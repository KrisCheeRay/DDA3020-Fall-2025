# Kaggle Hull Tactical Market Prediction - End-to-End Pipeline

## ðŸŽ¯ Key Results

| Model | Score | Description |
|-------|-------|-------------|
| **Online Learning MLP** | **2.6** | Simple MLP with online learning - **Best performing** |
| GRPO V1 | 0.43 | Complex RL-based strategy with N-HiTS + Policy Head |
| GRPO V2 | <0.43 | Improved reward function, worse performance |

**Key Insight**: Simple online learning outperformed complex pre-trained RL models, highlighting the importance of adaptability in changing market conditions.

## Overview

This project implements a robust, end-to-end pipeline for the Kaggle Hull Tactical Market Prediction competition. The core strategy combines **Supervised Fine-Tuning (SFT)** of state-of-the-art time series models (N-HiTS, PatchTST) with a **Reinforcement Learning (RL)** inspired policy optimization stage (GRPO).

**Note**: While the GRPO approach is theoretically sound, the best results came from a simpler online learning MLP (`online_learning_simple.py`).

### Key Features

1.  **Advanced Time Series Models**: Utilizes **N-HiTS** (Neural Hierarchical Interpolation for Time Series) for base signal generation.
2.  **Robust Data Pipeline**: Implements a consistent `FeatureStore` for data cleaning, scaling, and feature management across training and inference.
3.  **GRPO (Generalized Reinforcement Policy Optimization)**: Trains a lightweight Policy Head (MLP) to optimize trading positions based on base model signals and market state (volatility, trend).
4.  **"Learn from Mistakes"**: The GRPO stage is trained on Out-of-Sample (OOS) predictions ("rotten apples") generated by the SFT models, ensuring the policy learns to handle realistic prediction errors.
5.  **Kaggle-Ready Inference**: Includes a complete, optimized inference runtime (`starter_notebook.py`) that handles cold starts, batch processing, and offline package installation.

## Pipeline Architecture & File Responsibilities

### 0) Environment & Dependencies
-   `requirements.txt`: List of required Python packages.
-   **Note**: For Kaggle submission, we use offline wheel files for `neuralforecast` and its dependencies.

### 1) Data Formatting & Feature Consistency (FeatureStore)
-   **Goal**: Standardize raw data into the long format required by NeuralForecast and persist feature processing logic.
-   **Files**:
    -   `src/configs.py`:
        -   `DataSchema`: Defines column names (`unique_id`, `ds`, `y`, features).
        -   `SFTConfig`: Hyperparameters for SFT training.
        -   `ArtifactPaths`: Paths for saved models and artifacts.
    -   `src/feature_store.py`:
        -   `fit_transform(df)`: Fits scaler on training data and saves `scaler.pkl`, `features.json`.
        -   `transform(df)`: Reuses the saved scaler for inference, ensuring consistency.
    -   **Artifacts** (saved to `models/`):
        -   `models/scaler.pkl`: `StandardScaler` object.
        -   `models/features.json`: Feature column order metadata.

### 2) SFT Training & Signal Generation
-   **Goal**:
    1.  Train base models (N-HiTS) for signal generation.
    2.  **Generate OOS Signals**: Create "rotten apple" data (realistic noisy predictions) for the RL stage.
-   **Scripts**:
    -   `kaggle/work/sft_nhits.py`: Trains the N-HiTS model on the full dataset.
    -   `kaggle/work/gen_sft_signals.py`: Generates out-of-sample predictions using cross-validation logic.
-   **Artifacts**:
    -   `models/nhits_checkpoints/`: Saved N-HiTS model state.
    -   `models/sft_y_hat_oos.parquet`: OOS predictions for GRPO training.

### 3) GRPO Policy Training ("Eating Rotten Apples")
-   **Goal**: Train a Policy Head (MLP) that takes base model signals + market features and outputs a trading position [0, 2].
-   **Logic**: Trained on OOS predictions (`y_hat`) to adapt to model errors ("Train as you serve").
-   **Scripts**:
    -   `src/policy_head.py`: Defines the MLP architecture (Policy Network).
    -   `kaggle/work/train_grpo.py`: Loads `sft_y_hat_oos.parquet`, constructs the state vector, and trains the MLP using a custom reward function (PnL - Risk - Turnover).
-   **Artifacts**:
    -   `models/policy_head_best.pt`: Best weights for the Policy Head.

### 4) Inference Runtime (Kaggle Submission)
-   **Goal**: Load all artifacts and serve predictions efficiently within Kaggle's constraints.
-   **File**: `kaggle/work/starter_notebook.py` (This is the file to copy-paste into Kaggle).
    -   **`load_artifacts()`**: Loads FeatureStore, N-HiTS model, and PolicyHead.
    -   **`predict()`**:
        1.  Handles cold start (initializes history from training data or uses fallback).
        2.  Prepares rolling window.
        3.  Runs N-HiTS inference -> `y_hat`.
        4.  Constructs state vector (y_hat + features + volatility/trend).
        5.  Runs PolicyHead -> Final Position.
-   **Features**:
    -   **Offline Installation**: Robustly installs `neuralforecast` from uploaded wheel files.
    -   **Cold Start Strategy**: Uses momentum-based logic if history is insufficient.
    -   **Auto-Recovery**: Initializing history from `train.csv` for immediate performance.

### 5) Deployment to Kaggle
-   **Goal**: Package artifacts for the Kaggle environment.
-   **Artifacts to Upload** (to a private Kaggle Dataset):
    -   `models/scaler.pkl`
    -   `models/features.json`
    -   `models/nhits_checkpoints.zip` (Zipped directory)
    -   `models/policy_head_best.pt`
    -   `neuralforecast` wheel files (for offline installation)

## Directory Structure

```
Kaggle_Hull_Tactical/
â”œâ”€â”€ src/                        # Core source code
â”‚   â”œâ”€â”€ configs.py              # Configuration & Schema
â”‚   â”œâ”€â”€ feature_store.py        # Data processing
â”‚   â””â”€â”€ policy_head.py          # RL Policy Network
â”œâ”€â”€ kaggle/
â”‚   â”œâ”€â”€ work/                   # Execution scripts
â”‚   â”‚   â”œâ”€â”€ starter_notebook.py # MAIN SUBMISSION FILE
â”‚   â”‚   â”œâ”€â”€ sft_nhits.py        # Base model training
â”‚   â”‚   â”œâ”€â”€ gen_sft_signals.py  # Signal generation
â”‚   â”‚   â””â”€â”€ train_grpo.py       # RL Policy training
â”‚   â””â”€â”€ input/                  # Data directory (Kaggle structure)
â””â”€â”€ models/                     # Saved artifacts (local)
```

## Quick Start

1.  **Install Dependencies**: `pip install -r requirements.txt`
2.  **Train Base Model**: `python kaggle/work/sft_nhits.py`
3.  **Generate Signals**: `python kaggle/work/gen_sft_signals.py`
4.  **Train Policy**: `python kaggle/work/train_grpo.py`
5.  **Local Test**: `python kaggle/work/starter_notebook.py`
6.  **Deploy**: Upload `models/` to Kaggle Dataset and run `starter_notebook.py` on Kaggle.
